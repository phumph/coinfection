---
title: "DADA2 pipeline for 16S MiSeq data"
author: "Parris T Humphrey"
date: "8 Nov 2017"
output: html_document
---

Here we record the steps for processing demultiplexed 16S sequencing reads, in the form of `.fastq` files, and calling sub-OTUs (sOTUs) via the `DADA2` algorithm (Callahan et al. 2016). We will briefly review the theory in the supplemental methods, but here our main objective is to carry out the QC and sOTU assignment.

First we have to install and load the relevant **R** packages
```{r, message=FALSE, warning=FALSE, include=FALSE}
# source("https://bioconductor.org/biocLite.R")

biocLite("devtools")
library("devtools")
devtools::install_github("benjjneb/dada2")
library(dada2); packageVersion("dada2")
library(ShortRead); packageVersion("ShortRead")
library(ggplot2)
library(gridExtra)
library(bayesplot)
setwd("/Users/phumph/Dropbox/Phyllosphere_project/analysis_phy/16S_seq/demult/out") # CHANGE ME to location of file
```

Here we will focus only on the EL and NPB samples. We need to first pull out forward and reverse fastq files:
```{r}
# collect .fastq files
path <- "/Users/phumph/Dropbox/Phyllosphere_project/analysis_phy/16S_seq/demult/out" # CHANGE ME to the directory containing the fastq files after unzipping.
fnFs <- sort(list.files(path, pattern="-R1.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="-R2.fastq", full.names = TRUE))

# Get sample names
#sample.names <- sapply(strsplit(fnFs, "/"), tail, n=1) # remove directory information from sample names
#sample.names <- sapply(sample.names, function(x) paste(unlist(strsplit(x, "-"))[1]))
sample.names <- sapply(strsplit(basename(fnFs), "-"), `[`, 1)
```

Let's look at sequence error profile:
```{r}
# export quality profiles of N=n samples:
n <- 50
ns <- sample(length(fnFs),n)
the_F_files <- fnFs[ns]
the_R_files <- fnRs[ns]
the_names <- sample.names[ns]
outpath <- '/Users/phumph/Dropbox/Phyllosphere_project/analysis_phy/16S_seq/demult/dada2_v1/QC/'

for(fil in 1:length(the_F_files)) {

  QP1 <- plotQualityProfile(the_F_files[fil])
  QP2 <- plotQualityProfile(the_R_files[fil])

  png(filename = paste0(outpath,"QProf.",the_names[fil], ".png"), width = 640, height = 960)
    grid.arrange(QP1,QP2,nrow=2)
  dev.off()
}
# took about 3 min to run on 50 samples.
```

Perform filtering and trimming:
```{r message=FALSE, warning=FALSE}
#filtpath <- "/Users/phumph/Dropbox/Phyllosphere_project/analysis_phy/16S_seq/filt/"

filt_path <- file.path(path, "filtered") # Place filtered files in filtered/ subdirectory
filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq"))
filtRs <- file.path(filt_path, paste0(sample.names, "_R_filt.fastq"))

#filtFs <- paste0(filtpath, sapply(strsplit(fnFs, "\\."), `[`, 1), "_filt.fastq") # fix this
#filtRs <- paste0(filtpath, sapply(strsplit(fnRs, "\\."), `[`, 1), "_filt.fastq") # fix this

# need to supply full path:
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(150,150),
              maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
              compress=FALSE, multithread=TRUE)
#head(out)

write.csv(out, file = "post_filt_read_counts_17Nov.csv",quote=F)
# for(i in seq_along(fnFs)) {
#   system.time(fastqPairedFilter(paste0(path, c(fnFs[i], fnRs[i])), c(filtFs[i], filtRs[i]), maxN=0, maxEE=2, truncQ=2, compress=TRUE, verbose=TRUE))
# }
```

Next we let `dada2` learn the base-wise error profiles and plot them:
```{r}
outpath <- '/Users/phumph/Dropbox/Phyllosphere_project/analysis_phy/16S_seq/demult/dada2_v1/QC/'

# remove sequence files with < 100 reads
removes <- row.names(as.data.frame(out)[out[,'reads.out'] < 4000,])
filtFs <- filtFs[!filtFs %in% grep(paste(removes, collapse="|"),filtFs, value = TRUE)]
filtRs <- filtRs[!filtRs %in% grep(paste(removes, collapse="|"),filtRs, value = TRUE)]

# calibrate the base-wise error model for fwd and rev reads
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)

# plot error profiles and parametric fit to data
EP1 <- plotErrors(errF, nominalQ=TRUE)
EP2 <- plotErrors(errR, nominalQ=TRUE)

# save file to QC outpath
png(filename = paste0(outpath,"EProf.png"), width = 640, height = 960)
  grid.arrange(EP1,EP2,nrow=2)
dev.off()
```

Next we dereplicate the filtered fastq files, and then perform the `dada2` algorithm to collapse sequences into amplicon sequence variants (ASVs):
```{r message=FALSE}
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)
# Name the derep-class objects by the sample names
names(derepFs) <- sample.names
names(derepRs) <- sample.names

dadaFs <- dada(derepFs, err=errF, pool=TRUE, selfConsist=FALSE, multithread=TRUE) # 60m
dadaRs <- dada(derepRs, err=errR, pool=TRUE, selfConsist=FALSE, multithread=TRUE) # 40m

mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs)
seqtab.all <- makeSequenceTable(mergers)
#bim <- isBimeraDenovo(seqtab.all, verbose=TRUE)
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
#seqtab <- seqtab.all[,!bim]

write.csv(seqtab.nochim, file="dada2_ASV_table_v3.csv", quote=F)

# track reads through pipeline
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(mergers, getN), rowSums(seqtab), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoised", "merged", "tabled", "nonchim")
rownames(track) <- sample.names
write.csv(track,file = "seqs_through_pipe_v1.csv",quote=F)
```

Assign taxonomy to all inferred ASVs, using the Ribosomal Database Project (RDP) naive Bayesian classifier, implemented in dada2:
```{r}
taxa <- assignTaxonomy(seqtab.nochim, "silva_nr_v128_train_set.fa.gz", multithread=TRUE, tryRC=TRUE)
# if lots of NAs, may need to revcomp the refdb. Use
taxa <- addSpecies(taxa, "silva_species_assignment_v128.fa.gz")

# export for the inspection
write.csv(taxa, file="dada2_taxa_Silva_wSpp_v1.csv", quote=F)
```

Let's use the RDP identities to assign each ASV a unique lowest-level taxonomic ID:
```{r}
taxa <-  read.csv(file = "dada2_taxa_Silva_wSpp_v1.csv", row.names = 1)
# run through taxa dataset; find lowest non-NA label; if exists, append integer.
find.lowest.taxon <- function(theDF){
  taxa.lowest <- data.frame(theDF)
  #taxa.lowest[,'unique.id'] <- NA
  for(t in 1:length(taxa.lowest[,1])){
    int <- 0

    # find non-NA entry in taxon vector:
    taxa.tmp <- taxa.lowest[t,][paste(taxa.lowest[t,]) != "NA"]

    if (length(taxa.tmp[1,])==0){
      taxa.lowest[t,'unique.id'] <- "none_"
    } else{ # if length of non-NA hits > 0
      # paste lowest-level non-NA hit to unique.id column
      taxa.lowest[t,'unique.id'] <- paste0(taxa.tmp[[length(taxa.tmp)]])
      # now append species information if indeed this was found
      if (!is.na(taxa.lowest[t,'Species'])){
        taxa.lowest[t,'unique.id'] <- paste0(taxa.tmp[[length(taxa.tmp)-1]],
                                             '.',
                                             paste0(taxa.lowest[t,'Species']))  
      }
    }  

    # just add unique integer to end of unique.id:
    taxa.lowest[t,'unique.id'] <- paste0(taxa.lowest[t,'unique.id'],'_',t)
    # no matter what the unique.id designation, determine whether it is a redundant label;
    # if so, add integer
    # if(length(grep(taxa.lowest[t,'unique.id'],taxa.lowest[,'unique.id'])) > 1){
    #     int <- length(grep(taxa.lowest[t,'unique.id'],taxa.lowest[,'unique.id']))
    #     taxa.lowest[t,'unique.id'] <- paste0(taxa.lowest[t,'unique.id'],int+1)
    #     # } else{
    #     #   taxa.lowest[t,'unique.id'] <- paste0(taxa.lowest[t,'unique.id'],1)
    #     # }
    #   }
    }
  return(taxa.lowest)
}

taxa2 <- find.lowest.taxon(taxa)
write.csv(taxa2, file = "dada2_taxa_Silva_wSpp_uniques_v1.csv", quote = F)
```
